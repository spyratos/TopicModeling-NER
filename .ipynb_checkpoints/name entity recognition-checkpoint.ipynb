{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags \n",
    "import os\n",
    "import collections\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pickle\n",
    "from collections import Iterable\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the GMB corpus file containing the annotated text without considering the subcategories in annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({u'O': 1146068, u'geo': 58388, u'org': 48094, u'per': 44254, u'tim': 34789, u'gpe': 20680, u'art': 867, u'eve': 709, u'nat': 300})\n",
      "Words= 1354149\n"
     ]
    }
   ],
   "source": [
    "ner_tags = collections.Counter()\n",
    "corpus_root = \"gmb-2.2.0\"   # Make sure you set the proper path to the unzipped corpus\n",
    "for root, dirs, files in os.walk(corpus_root):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".tags\"):\n",
    "            with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                file_content = file_handle.read().decode('utf-8').strip()\n",
    "                annotated_sentences = file_content.split('\\n\\n')   # Split sentences\n",
    "                for annotated_sentence in annotated_sentences:\n",
    "                    annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]  # Split words\n",
    " \n",
    "                    standard_form_tokens = []\n",
    " \n",
    "                    for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                        annotations = annotated_token.split('\\t')   # Split annotation\n",
    "                        word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    " \n",
    "                        # Get only the primary category\n",
    "                        if ner != 'O':\n",
    "                            ner = ner.split('-')[0]\n",
    " \n",
    "                        ner_tags[ner] += 1\n",
    " \n",
    "print ner_tags\n",
    "print \"Words=\", sum(ner_tags.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the NE Chunker\n",
    "## Create proper format for classifier\n",
    "### Feature extraction (with history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features(tokens, index, history):\n",
    "\n",
    " \n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START2]', '[START2]'), ('[START1]', '[START1]')] + list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]')]\n",
    "    history = ['[START2]', '[START1]'] + list(history)\n",
    " \n",
    "    # shift the index with 2, to accommodate the padding\n",
    "    index += 2\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    previob = history[index - 1]\n",
    "    contains_dash = '-' in word\n",
    "    contains_dot = '.' in word\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    " \n",
    "    allcaps = word == word.capitalize()\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    " \n",
    "    prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    nextallcaps = prevword == prevword.capitalize()\n",
    "    nextcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "        'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    " \n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    " \n",
    "        'prev-iob': previob,\n",
    " \n",
    "        'contains-dash': contains_dash,\n",
    "        'contains-dot': contains_dot,\n",
    " \n",
    "        'all-caps': allcaps,\n",
    "        'capitalized': capitalized,\n",
    " \n",
    "        'prev-all-caps': prevallcaps,\n",
    "        'prev-capitalized': prevcapitalized,\n",
    " \n",
    "        'next-all-caps': nextallcaps,\n",
    "        'next-capitalized': nextcapitalized,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utility functions to help us with the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_conll_iob(annotated_sentence):\n",
    "    \"\"\"\n",
    "    `annotated_sentence` = list of triplets [(w1, t1, iob1), ...]\n",
    "    Transform a pseudo-IOB notation: O, PERSON, PERSON, O, O, LOCATION, O\n",
    "    to proper IOB notation: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n",
    "    \"\"\"\n",
    "    proper_iob_tokens = []\n",
    "    for idx, annotated_token in enumerate(annotated_sentence):\n",
    "        tag, word, ner = annotated_token\n",
    " \n",
    "        if ner != 'O':\n",
    "            if idx == 0:\n",
    "                ner = \"B-\" + ner\n",
    "            elif annotated_sentence[idx - 1][2] == ner:\n",
    "                ner = \"I-\" + ner\n",
    "            else:\n",
    "                ner = \"B-\" + ner\n",
    "        proper_iob_tokens.append((tag, word, ner))\n",
    "    return proper_iob_tokens\n",
    " \n",
    " \n",
    "def read_gmb(corpus_root):\n",
    "    for root, dirs, files in os.walk(corpus_root):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".tags\"):\n",
    "                with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                    file_content = file_handle.read().decode('utf-8').strip()\n",
    "                    annotated_sentences = file_content.split('\\n\\n')\n",
    "                    for annotated_sentence in annotated_sentences:\n",
    "                        annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]\n",
    " \n",
    "                        standard_form_tokens = []\n",
    " \n",
    "                        for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                            annotations = annotated_token.split('\\t')\n",
    "                            word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    " \n",
    "                            if ner != 'O':\n",
    "                                ner = ner.split('-')[0]\n",
    " \n",
    "                            if tag in ('LQU', 'RQU'):   # Make it NLTK compatible\n",
    "                                tag = \"``\"\n",
    " \n",
    "                            standard_form_tokens.append((word, tag, ner))\n",
    " \n",
    "                        conll_tokens = to_conll_iob(standard_form_tokens)\n",
    " \n",
    "                        # Make it NLTK Classifier compatible - [(w1, t1, iob1), ...] to [((w1, t1), iob1), ...]\n",
    "                        # Because the classfier expects a tuple as input, first item input, second the class\n",
    "                        yield [((w, t), iob) for w, t, iob in conll_tokens]\n",
    " \n",
    "reader = read_gmb(corpus_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train of Classifier\n",
    "\n",
    "We are going to train a ClassifierBasedTagger model wich uses a NaiveBayes classifies for predicting sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NamedEntityChunker(ChunkParserI):\n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        assert isinstance(train_sents, Iterable)\n",
    " \n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=train_sents,\n",
    "            feature_detector=features,\n",
    "            **kwargs)\n",
    " \n",
    "    def parse(self, tagged_sent):\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    " \n",
    "        # Transform the result from [((w1, t1), iob1), ...] \n",
    "        # to the preferred list of triplets format [(w1, t1, iob1), ...]\n",
    "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
    " \n",
    "        # Transform the list of triplets to nltk.Tree format\n",
    "        return conlltags2tree(iob_triplets)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training samples = 55809\n",
      "#test samples = 6201\n"
     ]
    }
   ],
   "source": [
    "reader = read_gmb(corpus_root)\n",
    "data = list(reader)\n",
    "training_samples = data[:int(len(data) * 0.9)]\n",
    "test_samples = data[int(len(data) * 0.9):]\n",
    " \n",
    "print \"#training samples = %s\" % len(training_samples)    # training samples = 55809\n",
    "print \"#test samples = %s\" % len(test_samples)                # test samples = 6201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 35s, sys: 1.92 s, total: 1min 37s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chunker = NamedEntityChunker(training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  'm/VBP\n",
      "  going/VBG\n",
      "  to/TO\n",
      "  (geo Germany/NNP)\n",
      "  this/DT\n",
      "  (tim Monday/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print chunker.parse(pos_tag(word_tokenize(\"I'm going to Germany this Monday.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.920860617399\n"
     ]
    }
   ],
   "source": [
    "score = chunker.evaluate([conlltags2tree([(w, t, iob) for (w, t), iob in iobs]) for iobs in test_samples[:500]])\n",
    "print score.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
